# ğŸ§  RAG Question Answering System (PyTorch + FAISS)

A production-ready Retrieval-Augmented Generation (RAG) pipeline built with PyTorch, FAISS, and FastAPI.

This project demonstrates how to design, build, and deploy a scalable semantic search + LLM generation system â€” the core pattern behind modern AI assistants and enterprise knowledge bots.

---

## ğŸš€ Features

* ğŸ” Semantic document search using FAISS
* ğŸ§© Intelligent chunking pipeline
* ğŸ§  Transformer-based embeddings
* ğŸ’¬ Retrieval-augmented generation
* âš¡ FastAPI inference service
* ğŸ³ Fully containerized with Docker
* ğŸ§ª Modular, production-style codebase

---

## ğŸ—ï¸ Architecture

```
User Query
   â†“
Embed Query
   â†“
FAISS Vector Search
   â†“
Retrieve Top-K Chunks
   â†“
Generator (LLM)
   â†“
Final Answer
```

### Components

| Component    | Responsibility                        |
| ------------ | ------------------------------------- |
| Chunker      | Splits documents into semantic chunks |
| Embedder     | Converts text â†’ dense vectors         |
| Vector Store | Fast similarity search with FAISS     |
| Retriever    | Fetches relevant context              |
| Generator    | Produces final answer                 |
| API          | Exposes inference endpoint            |

---

## ğŸ“¦ Project Structure

```
kratoshi_RAG/
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ generator/
â”‚   â””â”€â”€ api/
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

### 1ï¸âƒ£ Clone repo

```bash
git clone https://github.com/kratoshi/kratoshi_RAG
cd kratoshi_RAG
```

---

### 2ï¸âƒ£ Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate  # mac/linux
```

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Running Locally

### Run pipeline test

```bash
python main.py
```

---

### Start API server

```bash
uvicorn src.api.main:app --reload
```

API docs:

```
http://127.0.0.1:8000/docs
```

---

## ğŸ³ Docker

### Build image

```bash
docker build -t kratoshi-rag .
```

---

### Run container

```bash
docker run -p 8000:8000 kratoshi-rag
```

---

## ğŸ”Œ API Usage

### Health check

```bash
GET /health
```

---

### Upload Documents

```bash
POST /ingest        # Only supports .txt
```

---

### Query endpoint

```bash
POST /query
```

**Request**

```json
{
  "query": "What is machine learning?"
}
```

**Response**

```json
{
  "answer": "...generated answer..."
}
```

---

## ğŸ§ª Example Workflow

1. Load documents
2. Chunk text
3. Generate embeddings
4. Build FAISS index
5. Query via API
6. Generate answer

---

## ğŸ”® Future Improvements

* [ ] Streaming responses
* [ ] Hybrid search (BM25 + vectors)
* [ ] Evaluation pipeline
* [ ] Batch indexing pipeline
* [ ] Kubernetes deployment
* [ ] Caching layer
* [ ] Observability (Prometheus/Grafana)


